# Coarse-GrainedWSD
 Word sense disambiguation (WSD) in Natural Language Processing (NLP) is the problem of identifying which “sense” (meaning) of a word is activated by the use of the word in a particular context or scenario.



First of all, we install the “Transformers” with this line of code:
To read and employ the Bert model, import the modules, and add the Transformers modules.
After that, I mount the Drive and navigate to the right place with the address and path. In the following two sections, I defined two functions. One of them is read_map_data, and the other is read_files.
The data we require is in a file called a data map that is right here. When a word appears in a sentence, we want to know both its literal meaning and the meaning to which it is linked. The RACE word is used as an example in the exercise; we need to figure out which of the several instances this word fits into. Here we take
advantage of the coarse_grained, and fine_grained options from the data set. Jason files have forms of nested dictionaries that we read and input values for. Here, we construct the mapper, which reads the files. The values are on the other side, and the keys are the keys found within our principal dataset, which we refer to as a Candidate Key for each of our samples.
   
 Here is related to the read file that we see again, there may be several words for a sentence that we want to know which meaning is defined, and we consider a separate sample for each word. So, if a sentence has three words, we take it as three samples, and the Lemmas and word values fixed. Candidate_sents are the items up for alteration. We take these out and produce a new data frame.
 A train_df and a test_df exist here. Then we apply the tokenizer.
We will include the words in the Bert model since we want to transform them into tokens. As a result, we may view the Bert model's output in the embedded form.

In the next part, there are defined functions. In the get_ word_vector sections, it takes a sentence; then turns them into an encoded and a tokenized, then looks for the index of the desired word, which uses get_word_2_indices. For example, we are supposed to see how many indices the RACE word has in the sentence. Then we encode the input sentence and put it in my Bert model, and it gives an output then.
If our encoded sentence has 30 words, then index 10 is the one we need. The model's output from the last hidden state section is a matrix of 760 columns, which corresponds to the output number, and 760 elements, one for each word. We have 30 vectors of 760 if our phrase has 30 encoded or tokenized values. These vectors are all the results of the Bert. Which of these should we apply right now? Its index matches the one we desire.
Now, once again, for each of the meanings that we obtained before, we put it into the Bert again, and it gives us an output, and this time we get 760 output meanings, the difference here is that we get the whole sentence, not just a specific word.
We use similarity, which becomes the model similarity function, for each word or output, that was more similar to our word output. For these, it puts "for" to see which one is closer. And since this is the portion of the algorithm that requires a longer run time, we will apply it to the entire dataset. The output of this gives us a series of scores that we put these scores into the SoftMax function and normalize somehow.
If we look at the candidate data, the maximum value in this section's data is 15, and we enter that value for candidates with significance and zero for those without. For instance, if the race word has three meanings, we have three values, and we write zero for the other twelve parts. The model similarity function takes all of these into consideration.
When we use it, the complete dataset is applied, and each sample receives a score of 15. The label on the photos is likewise the same (15), making it a one-hot vector with the meaning that one is assigned to it,

and the rest is zero. Due to the fact that two Pickle files—train_df and test_df—are saved and kept in the data folder, this is done on train_df and test_df, which likewise has an extremely long runtime.
Here, two columns are added to the test_df and train_df file, one of which is param, which is extracted from these two things, one of which is model_pred and the other is C_words (Candidate Words).
The first output is calculating similarity, which has two productions itself. One of which is keys, and the other is scores. That score is soft max, and keys are the key exists in candidate keys; it is practically empty if it does not live. In this section, we save the files; then load them and create a dataset class that inherits from the torch dataset class.
The _get item_ is the main part of the work. I take several inputs, such as pred, candidates, and senses_word, then our label becomes candidates. index (senses_word), so convert it into one hot, and we have a torch. tensor(pred) is our “model predict” that would be the input of our model.
We create a model called T_model that has two layers, the first layer is a linear layer that has 15 inputs and 50 outputs, and the second layer converts 50 to the same 15. The first activation function is “relu” and the second layer is “softmax”.
We define the “train dataset”, which can be the most train_df dataset. Then we define the data loader, which can be the data loader of the “train dataset” with a batch size of 64. We use "Adam's optimizer" for five epochs,and our learning rate is ten to the power of minus three.
We considered the desired value for the loss function as cross-entropy because we have 15 outputs, so we need to use cross-entropy. Our Total Step is our Len train_data Loader. We consider the number of steps, input, and labels for each epoch. Our input is the “pred” of the model that we got, and it is the similarity score. Then we give to the model and calculate the value of the loss function.

“loss. backward”, “optimizer. step”, and “optimizer. zero_grad”, which gives us a “pred” in which there is a vector of 15, and that side becomes the number of batch size, 64 x 15, as an example. We will take this vector as an “argmax”, that 15, turns to one to see which index it is for, and we will do the same thing for the label so that it becomes a single number.
Our correct value is where "pred" is equal to the label. Then we count its number, and our accuracy becomes 100*(correct/len(train_pred)). Finally, we print it, after which we repeat the process for each of the data on our data test.
We give the data to the model, the model predicts for us, and we apply the argmax to an output of the model, then we compare it with the existing label, then employ the argmax to the label, and we put the output value into t_pred. So also put the label output into t_label. We calculate the accuracy score and the number of places where t_pred and t_label are equal to each other. Finally, it prints that value equal to 68%.
